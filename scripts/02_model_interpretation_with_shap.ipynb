{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP를 이용한 모델 예측 결과 해석\n",
        "\n",
        "SHAP 라이브러리를 사용하여 모델이 **'왜' 특정 플레이어를 치터로 판단했는지** 그 근거를 시각적으로 설명\n",
        "\n",
        "---\n",
        "\n",
        "### 주요 동작 흐름\n",
        "\n",
        "1.  **모델 및 데이터 로드**:\n",
        "    * 학습한 XGBoost 모델을 로드\n",
        "    * 분석할 대상의 데이터를 로드\n",
        "2.  **SHAP 값 계산**:\n",
        "    * 불러온 모델과 데이터를 사용하여 SHAP Explainer를 생성\n",
        "    * Explainer를 통해 각 특징이 모델 예측에 미친 영향력을 계산\n",
        "3.  **결과 분석 및 시각화**:\n",
        "    * 단일 예측에 대해 각 특징이 긍정적(치터일 확률을 높임) 또는 부정적(사람일 확률을 높임)으로 얼마나 기여했는지 시각화\n",
        "    * 전체 데이터셋에 걸쳐 어떤 특징이 전역적으로 가장 중요한 영향력을 가졌는지 시각화\n",
        "\n",
        "---\n",
        "\n",
        "### 핵심 포인트\n",
        "\n",
        "* **모델 투명성 확보**: 모델이 단순히 '치터다/아니다'라는 결과만 보여주는 것을 넘어, 어떤 특성이 그러한 판단의 결정적인 근거가 되었는지 설명\n",
        "* **시각적 근거 제시**: SHAP 플롯을 통해 모델의 복잡한 내부 동작을 직관적으로 이해할 수 있는 시각 자료를 생성"
      ],
      "metadata": {
        "id": "UAXHtkgUFC-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "hcImL1zZGrao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "20lNrahMGtqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import shap\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from scipy.stats import entropy"
      ],
      "metadata": {
        "id": "e10wNpb_G4Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# JavaScript 시각화를 위해 초기화 (Colab에서 필수)\n",
        "shap.initjs()"
      ],
      "metadata": {
        "id": "fPmUU8qmbR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 및 데이터 로드"
      ],
      "metadata": {
        "id": "k5OK11Q8Gait"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 불러올 모델의 경로 지정 ---\n",
        "model_load_path = '/content/drive/MyDrive/Preprocessed_Dataset/xgboost_cheater_detection_model.json'\n",
        "\n",
        "# --- 모델 로드 ---\n",
        "model = xgb.XGBClassifier()\n",
        "model.load_model(model_load_path)"
      ],
      "metadata": {
        "id": "T3RaT0a6GpTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 불러올 데이터의 경로 지정 ---\n",
        "human_data_path = \"/content/drive/MyDrive/Preprocessed_Dataset/aim_analysis_human.xlsx\" # 사람 데이터 엑셀 경로 지정\n",
        "cheat_data_path = \"/content/drive/MyDrive/Preprocessed_Dataset/aim_analysis_cheat.xlsx\" # 치트 데이터 엑셀 경로 지정\n",
        "\n",
        "# --- 엑셀 로드 ---\n",
        "df_human = pd.read_excel(human_data_path)\n",
        "df_cheat = pd.read_excel(cheat_data_path)\n",
        "\n",
        "# --- 치트 라벨 추가 ---\n",
        "df_human['cheat'] = 0 # 사람 데이터프레임에 모든 행에 'cheat' 컬럼을 추가하고 값을 0으로 설정\n",
        "df_cheat['cheat'] = 1 # 치트 데이터프레임의 모든 행에 'cheat' 컬럼을 추가하고 값을 1로 설정"
      ],
      "metadata": {
        "id": "xE0NtPHiG-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 데이터프레임을 하나로 합침\n",
        "df_concat = pd.concat([df_human, df_cheat], ignore_index=True)\n",
        "\n",
        "# 'filename'을 기준으로 데이터를 그룹화\n",
        "df_grouped = df_concat.groupby('filename')\n",
        "\n",
        "print(f\"총 {len(df_grouped)}개의 고유한 파일이 있습니다.\")"
      ],
      "metadata": {
        "id": "1tIAvt_qbaIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 데이터 전처리 ---\n",
        "\n",
        "'''\n",
        "01_training_xgboost_model.ipynb와 동일하게 데이터 전처리를 진행\n",
        "'''\n",
        "\n",
        "# 파라미터 설정\n",
        "WINDOW_SIZE = 150  # 150 프레임 (5초)\n",
        "STEP_SIZE = 30     # 30 프레임 (1초) 만큼 이동 (120 프레임 겹침)\n",
        "ENTROPY_BINS = 10  # 엔트로피 계산 시 데이터를 나눌 구간(bin) 수\n",
        "\n",
        "# 요약할 원본 특징 리스트\n",
        "features_origin_list = ['dx', 'dy', 'velocity', 'acceleration', 'jerk', 'angle_deg', 'angle_change']\n",
        "\n",
        "# 요약 특징 청크(chunk)를 저장할 리스트\n",
        "all_video_chunks = []\n",
        "\n",
        "# 각 영상(파일)별로 반복 처리\n",
        "for filename, video_data in df_grouped:\n",
        "\n",
        "    # 해당 영상의 총 프레임 수\n",
        "    n_frames = len(video_data)\n",
        "\n",
        "    # 슬라이딩 윈도우 적용\n",
        "    # 0 프레임부터 (총 프레임 - 윈도우 크기)까지 STEP_SIZE 만큼 이동\n",
        "    # STEP_SIZE보다 적게 남은 데이터는 버림\n",
        "    for i in range(0, n_frames - WINDOW_SIZE + 1, STEP_SIZE):\n",
        "\n",
        "        # 150 프레임으로 구성된 현재 윈도우(청크)를 추출\n",
        "        chunk = video_data.iloc[i : i + WINDOW_SIZE]\n",
        "\n",
        "        # 현재 청크의 요약 특징을 저장할 딕셔너리\n",
        "        summary_stats = {}\n",
        "\n",
        "        # 기본 정보(파일이름, 치트여부) 추가\n",
        "        summary_stats['filename'] = filename\n",
        "        # cheat 값은 윈도우 내에서 동일하므로 첫 번째 값을 사용\n",
        "        summary_stats['cheat'] = chunk['cheat'].iloc[0]\n",
        "        summary_stats['start_frame'] = i\n",
        "\n",
        "        # --- 각 특징별로 요약 특징 계산 ---\n",
        "        for col in features_origin_list:\n",
        "            series = chunk[col] # 현재 청크의 해당 특징(Series)\n",
        "\n",
        "            # --- 요약 특징 계산 ---\n",
        "            summary_stats[f'{col}_mean'] = series.mean()          # 평균\n",
        "            summary_stats[f'{col}_median'] = series.median()      # 중앙값\n",
        "\n",
        "            # mode는 여러 개일 수 있으므로 첫 번째 값만 사용 (없으면 NaN)\n",
        "            mode_val = series.mode()\n",
        "            summary_stats[f'{col}_mode'] = mode_val.iloc[0] if not mode_val.empty else np.nan\n",
        "\n",
        "            s_max = series.max()\n",
        "            s_min = series.min()\n",
        "            summary_stats[f'{col}_range'] = s_max - s_min         # 범위\n",
        "            summary_stats[f'{col}_std'] = series.std()            # 표준편차\n",
        "\n",
        "            q75 = series.quantile(0.75)\n",
        "            q25 = series.quantile(0.25)\n",
        "            summary_stats[f'{col}_iqr'] = q75 - q25               # 사분위수 범위\n",
        "\n",
        "            summary_stats[f'{col}_max'] = s_max                   # 최댓값\n",
        "            summary_stats[f'{col}_min'] = s_min                   # 최솟값\n",
        "            summary_stats[f'{col}_skewness'] = series.skew()      # 왜도\n",
        "            summary_stats[f'{col}_kurtosis'] = series.kurtosis()  # 첨도\n",
        "\n",
        "            s_np = series.values\n",
        "            summary_stats[f'{col}_zcr'] = ((s_np[:-1] * s_np[1:]) < 0).sum() / WINDOW_SIZE  # 영점 교차율\n",
        "\n",
        "            # 데이터를 구간으로 나눔\n",
        "            binned_series = pd.cut(series, bins=ENTROPY_BINS, labels=False)\n",
        "            # 각 구간의 빈도수를 계산\n",
        "            value_counts = binned_series.value_counts(normalize=True)\n",
        "            summary_stats[f'{col}_entropy'] = entropy(value_counts)                         # 엔트로피\n",
        "\n",
        "        # --- 모든 특징 조합의 상관계수 계산 ---\n",
        "        for feat1, feat2 in itertools.combinations(features_origin_list, 2):\n",
        "            correlation = chunk[feat1].corr(chunk[feat2])\n",
        "\n",
        "            # 한쪽 값이 일정(std=0)하면 상관계수가 NaN이 됨 -> 0으로 처리\n",
        "            if pd.isna(correlation):\n",
        "                summary_stats[f'corr_{feat1}_{feat2}'] = 0\n",
        "            else:\n",
        "                summary_stats[f'corr_{feat1}_{feat2}'] = correlation\n",
        "\n",
        "        # 완성된 요약 특징 딕셔너리를 리스트에 추가\n",
        "        all_video_chunks.append(summary_stats)\n",
        "\n",
        "# 모든 청크 리스트를 하나의 새로운 DataFrame으로 변환\n",
        "df = pd.DataFrame(all_video_chunks)"
      ],
      "metadata": {
        "id": "oxpz4p65Zk5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 특징 및 라벨 분리 ---\n",
        "y_test_df = df['cheat'] # 'cheat' 열을 라벨로 사용\n",
        "groups = df['filename'] # 데이터를 나눌 기준 (파일 이름)\n",
        "non_feature_cols = ['filename', 'cheat', 'start_frame'] # 특징이 아닌 열들\n",
        "X_test_df = df.drop(columns=non_feature_cols)           # 구별자를 제외하여 데이터셋 생성\n",
        "\n",
        "print(\"--- 전체 데이터셋 (X_test_df) ---\")\n",
        "print(f\"총 데이터(청크) 수: {len(X_test_df)}\")\n",
        "print(f\"총 특징 수: {X_test_df.shape[1]}\")"
      ],
      "metadata": {
        "id": "omWqFAWabqMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP 값 계산"
      ],
      "metadata": {
        "id": "x90okprbGf8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SHAP Explainer 생성 ---\n",
        "explainer = shap.TreeExplainer(model)"
      ],
      "metadata": {
        "id": "edns5vjGGiBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SHAP 값 계산 ---\n",
        "shap_values = explainer.shap_values(X_test_df)"
      ],
      "metadata": {
        "id": "BkQHlCcwHINY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SHAP Interaction 값 계산 ---\n",
        "# 전체 데이터셋을 사용하면 계산량이 엄청나게 많아 매우 오래 걸릴 수 있음\n",
        "'''\n",
        "shap_interaction_values = explainer.shap_interaction_values(X_test_df)\n",
        "'''\n",
        "\n",
        "# 샘플링으로 데이터셋 줄이기\n",
        "X_test_sample = X_test_df.sample(100, random_state=42)\n",
        "shap_interaction_values_sample = explainer.shap_interaction_values(X_test_sample)"
      ],
      "metadata": {
        "id": "uGo7qx84ic_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 분석 및 시각화"
      ],
      "metadata": {
        "id": "8JbSoKP-GiYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 고유한 파일 이름 리스트에서 랜덤으로 1개 선택 ---\n",
        "unique_files = df['filename'].unique()\n",
        "rng = np.random.default_rng(seed=42)\n",
        "target_filename = rng.choice(unique_files)\n",
        "\n",
        "# --- 해당 파일에 속하는 '원본 df 기준 인덱스' 리스트 생성 ---\n",
        "TARGET_INDICES_LIST = df[df['filename'] == target_filename].index.tolist()\n",
        "num_chunks_in_file = len(TARGET_INDICES_LIST)\n",
        "\n",
        "# --- 선택된 파일 정보 출력 ---\n",
        "file_info = df.loc[TARGET_INDICES_LIST[0]] # 첫 번째 청크 정보로 라벨 확인\n",
        "actual_label = \"Cheater (1)\" if file_info['cheat'] == 1 else \"Human (0)\"\n",
        "\n",
        "print(f\"--- 랜덤 분석 대상 파일: {target_filename} ---\")\n",
        "print(f\"  총 청크 수: {num_chunks_in_file}\")\n",
        "print(f\"  실제 라벨: {actual_label}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"해당 파일의 청크 인덱스 (X_test_df 기준):\")\n",
        "print(TARGET_INDICES_LIST)"
      ],
      "metadata": {
        "id": "F-U_0AwJHMR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 분석할 청크의 '순번' 지정 ---\n",
        "CHUNK_ITERATOR_INDEX = 0\n",
        "\n",
        "# --- 유효성 검사 ---\n",
        "if 'TARGET_INDICES_LIST' not in locals():\n",
        "    print(\"오류: TARGET_INDICES_LIST 변수가 정의되지 않았습니다.\")\n",
        "elif CHUNK_ITERATOR_INDEX >= len(TARGET_INDICES_LIST) or CHUNK_ITERATOR_INDEX < 0:\n",
        "    print(f\"오류: 인덱스 {CHUNK_ITERATOR_INDEX}가 유효하지 않습니다.\")\n",
        "    print(f\"유효한 순번(ITERATOR) 범위: 0 ~ {len(TARGET_INDICES_LIST) - 1}\")\n",
        "else:\n",
        "    # --- 실제 데이터 인덱스 가져오기 ---\n",
        "    # 리스트에서 '순번'에 해당하는 '실제 인덱스'를 조회\n",
        "    actual_df_index = TARGET_INDICES_LIST[CHUNK_ITERATOR_INDEX]\n",
        "\n",
        "    # --- 해당 청크 정보 확인 ---\n",
        "    selected_chunk_info = df.loc[actual_df_index]\n",
        "    selected_start_frame = selected_chunk_info['start_frame']\n",
        "\n",
        "    print(f\"--- 파일 '{target_filename}'의 {CHUNK_ITERATOR_INDEX}번째 청크 분석 ---\")\n",
        "    print(f\"  (실제 인덱스: {actual_df_index} / 시작 프레임: {selected_start_frame})\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- SHAP Explanation 객체 생성 ---\n",
        "    # '전체' shap_values에서 'actual_df_index'번째 값만 사용\n",
        "    explanation_chunk = shap.Explanation(\n",
        "        values=shap_values[actual_df_index],\n",
        "        base_values=explainer.expected_value,\n",
        "        data=X_test_df.iloc[actual_df_index],\n",
        "        feature_names=X_test_df.columns\n",
        "    )"
      ],
      "metadata": {
        "id": "Zs2yU1ganGse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Waterfall Plot ---\n",
        "shap.plots.waterfall(explanation_chunk)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YZe9z6pCnCVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Force Plot ---\n",
        "shap.plots.force(explainer.expected_value,\n",
        "                 shap_values[actual_df_index],\n",
        "                 X_test_df.iloc[actual_df_index])"
      ],
      "metadata": {
        "id": "nzr2-1emjT7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bar Plot ---\n",
        "shap.plots.bar(explanation_chunk)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n3aUHKMkjVAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 모델 전체 경향성 분석 ---\n",
        "shap.summary_plot(shap_values, X_test_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9RnpygzdHO-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqKZ-ZnxjW5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터셋 버전\n",
        "'''\n",
        "shap.summary_plot(shap_interaction_values, X_test_df)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "# 샘플링 버전\n",
        "shap.summary_plot(shap_interaction_values_sample, X_test_sample)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCMGXYnjjXx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}